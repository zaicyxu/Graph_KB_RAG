# -*- coding: utf-8 -*-
#!/usr/bin/env python

"""
@Project Name: KnowledgeGraph_ExpertSystem
@File Name: Verify_KB.py
@Software: Python
@Time: Mar/2025
@Author: Yufei Quan, Rui Xu
@Version: 0.6.2
@Description: Verify the accuracy of the answers generated by the Expert System
              using structured ground truth comparison. This version only calls the Expert System.
              The final results are stored horizontally (one row per test).
"""

import csv
import re
from query_generation import QuestionGenerator
from main_knowledgebase_system import Neo4jExpertSystem
from porlog import configuration
from fuzzywuzzy import fuzz



class ExperimentRunner:
    def __init__(self, top_k=3, similarity_threshold=0.6, num_questions=50):
        """
        Initialize the experiment runner.
        """
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold
        self.num_questions = num_questions

        # Initialize the question generator and Expert System.
        self.question_generator = QuestionGenerator()
        self.kb_system = Neo4jExpertSystem(
            configuration.NEO4J_URI, configuration.NEO4J_USER, configuration.NEO4J_PASSWORD
        )

    def run_experiment(self, output_file="experiment_results_kb.csv"):
        """
        Run the experiment: generate questions, retrieve answers using the Expert System,
        compute ground truth from the database, and save the results.
        Each row corresponds to one test with the following columns:
          Question | Expert System Answer | Ground Truth | KB Exact Accuracy | Category
        The CSV writer is set to quote all fields to ensure each cell is confined.
        """
        questions_with_categories = self.question_generator.generate_questions(self.num_questions)

        with open(output_file, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            # Write header row
            writer.writerow([
                "Question",
                "KB System Answer",
                "Ground Truth",
                "KB Exact Accuracy",
                "Category"
            ])

            for question_entry in questions_with_categories:
                question, category = question_entry
                print(f"Processing: {question} (Category: {category})")

                # Retrieve entities using the Expert System
                entities_kb = self.kb_system.retrieve_relevant_entities(question)
                # Generate answer using the Expert System
                answer_kb = self.kb_system.generate_answer(question, entities_kb)
                # Generate ground truth from the knowledge base
                ground_truth = self._generate_ground_truth(question)
                # Calculate exact match accuracy using fuzzy matching
                exact_kb = self.compute_rouge_accuracy(answer_kb, ground_truth)

                # Remove newlines so that each field stays in one CSV cell
                kb_answer_str = "\n".join([f"({s})-[:{r}]->({o})" for s, r, o in answer_kb])
                kb_answer = kb_answer_str.replace("\n", " | ")
                ground_truth = " ; ".join(ground_truth)

                # Write the result as one row
                writer.writerow([
                    question,
                    kb_answer,
                    ground_truth,
                    exact_kb,
                    category
                ])

    def _generate_ground_truth(self, question):
        """
        Generate ground truth labels for a given question by querying the knowledge graph.
        The format is "Entity - Relationship -> Connected Entity".
        """
        entities = self.kb_system.extract_keywords(question)
        if not entities:
            print(f"[WARNING] No keywords extracted from `{question}`.")
            return set()

        ground_truth = set()
        with self.kb_system.driver.session() as session:
            for entity in entities:
                cypher_query = f"""
                MATCH (n)-[r]->(m)
                WHERE toLower(n.Name) CONTAINS toLower('{entity}')
                RETURN n.Name AS node_name, type(r) AS relation_name, m.Name AS connected_node
                LIMIT 20
                """
                result = session.run(cypher_query)
                for record in result:
                    ground_truth.add(f"{record['node_name']} - {record['relation_name']} -> {record['connected_node']}")
        if not ground_truth:
            print(f"[WARNING] No ground truth found for `{question}`.")
        return ground_truth

    def compute_rouge_accuracy(self, predicted_triples, ground_truth_triples, fuzzy_threshold=80):
        parsed_ground_truth = set()
        for triple in ground_truth_triples:
            match = re.match(r"^\s*([\w\-\.]+)\s*-\s*([\w\-]+)\s*->\s*([\w\-\.]+)\s*$", triple)
            if match:
                subj = match.group(1).strip().lower().replace("_", "")
                rel = match.group(2).strip().lower()
                obj = match.group(3).strip().lower().replace("_", "")
                parsed_ground_truth.add((subj, rel, obj))
            else:
                print(f"[WARNING] Invalid ground truth triple: {triple}")

        parsed_predicted = []
        for triple in predicted_triples:
            if isinstance(triple, (list, tuple)) and len(triple) == 3:
                subj = re.sub(r"[^\w\-\.]", "", str(triple[0])).strip().lower().replace("_", "")
                rel = re.sub(r"[^\w\-]", "", str(triple[1])).strip().lower()
                obj = re.sub(r"[^\w\-\.]", "", str(triple[2])).strip().lower().replace("_", "")
                parsed_predicted.append((subj, rel, obj))
            else:
                print(f"[WARNING] Invalid predicted triple format: {triple}")

        if not parsed_predicted:
            return 0.0

        match_count = 0
        for pred in parsed_predicted:
            if pred in parsed_ground_truth:
                match_count += 1
            else:
                best_score = max(
                    [(fuzz.ratio(pred[0], gt[0]) + fuzz.ratio(pred[1], gt[1]) + fuzz.ratio(pred[2], gt[2])) / 3
                     for gt in parsed_ground_truth]
                )
                if best_score >= fuzzy_threshold:
                    match_count += 1

        accuracy = match_count / len(parsed_predicted)
        return round(accuracy, 3)

    def close(self):
        """
        Close database connections.
        """
        self.question_generator.close()
        self.kb_system.close()


if __name__ == "__main__":
    experiment = ExperimentRunner(top_k=5, similarity_threshold=0.6, num_questions=20)
    experiment.run_experiment("experiment_results_kb.csv")
    experiment.close()
